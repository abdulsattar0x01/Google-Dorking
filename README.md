
# Google-Dorking
## What is a web crawler? | How web spiders work

A web crawler, or spider, is a type of bot that is typically operated by search engines like Google and Bing. Their purpose is to index the content of websites all across the Internet so that those websites can appear in search engine results.

These crawlers discover content through various means. One being by pure discovery, where a URL is visited by the crawler and information regarding the content type of the website is returned to the search engine. In fact, there are lots of information modern crawlers scrape â€“ but we will discuss how this is used later. Another method crawlers use to discover content is by following any and all URLs found from previously crawled websites. Much like a virus in the sense that it will want to traverse/spread to everything it can.

![4nrDDa0](https://user-images.githubusercontent.com/73302708/132658637-65ab27d3-63cf-465a-955c-ab4dde46f973.png)


## What is a web crawler bot?
